{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Evaluate Your Model with GLUE Benchmark\n",
    "\n",
    "**Author:** Chen Qian<br>\n",
    "**Date created:** 2023/01/01<br>\n",
    "**Last modified:** 2023/01/04<br>\n",
    "**Description:** Evaluate Your Model with GLUE Benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## What's This Guide\n",
    "\n",
    "This guide shows how to evaluate your model using GLUE benchmark. This guide\n",
    "covers the following topics:\n",
    "- Overview of GLUE benchmark.\n",
    "- Preprocessing GLUE dataset to unify the data format.\n",
    "- Making a model that works for all glue tasks.\n",
    "- Basic setup of the training (finetuning if you load a pretrained model) workflow.\n",
    "- Generate submission files and submit it to GLUE leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Why Do I Write This Guide\n",
    "\n",
    "I was trying to evaluate my model with GLUE benchmark, but surprisingly I found that\n",
    "despite the popularity of GLUE benchmark, there is no handy tool/tutorial that shows me\n",
    "how that can be achieved. One big question I have is - can I have a unified script that\n",
    "\"just works\" for all GLUE tasks? A followup is - can I generate the GLUE leaderboard\n",
    "submission file with the same script? Finally I wrote this script, and put it in KerasNLP\n",
    "github repo, please check it out at\n",
    "[https://github.com/keras-team/keras-nlp/tree/master/examples/glue_benchmark](https://gith\n",
    "ub.com/keras-team/keras-nlp/tree/master/examples/glue_benchmark). You can plug in any\n",
    "custom model into the script following the guidance, and it's fully compatible with GPU\n",
    "and TPU.\n",
    "\n",
    "While a runnable script is good, it cannot cover enough details without massive, tedious\n",
    "and unreadable comments. That's the reason for me to write this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Overview of GLUE Benchmark\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[GLUE benchmark](https://gluebenchmark.com/) is commonly used to test a model's\n",
    "performance at text understanding. It consists of 10 tasks:\n",
    "\n",
    "1. [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability): Predict\n",
    "if the sentence is grammatically correct.\n",
    "\n",
    "1. [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank):\n",
    "Predict the sentiment of a given sentence.\n",
    "\n",
    "1. [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft\n",
    "Research Paraphrase Corpus): Predict whether a pair of sentences are semantically\n",
    "equivalent.\n",
    "\n",
    "1. [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora\n",
    "Question Pairs2): Predict whether a pair of questions are semantically equivalent.\n",
    "\n",
    "1. [MNLI](http://www.nyu.edu/projects/bowman/multinli/) (Multi-Genre Natural Language\n",
    "Inference): Predict if the premise entails the hypothesis (entailment), contradicts the\n",
    "hypothesis (contradiction), or neither (neutral).\n",
    "\n",
    "1. [QNLI](https://rajpurkar.github.io/SQuAD-explorer/)(Question-answering Natural\n",
    "Language Inference): Predict if the context sentence contains the answer to the question.\n",
    "\n",
    "1. [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment)(Recognizing Textual\n",
    "Entailment): Predict if a sentence entails a given hypothesis or not.\n",
    "\n",
    "1. [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html)(Winograd\n",
    "Natural Language Inference): Predict if the sentence with the pronoun substituted is\n",
    "entailed by the original sentence.\n",
    "\n",
    "1. [AX](https://gluebenchmark.com/diagnostics)(Diagnostics Main): Evaluate sentence\n",
    "understanding through Natural Language Inference (NLI) problems.\n",
    "\n",
    "1. [STSB](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)(Semantic Textual\n",
    "Similarity Benchmark): Predict the similarity score between 2 sentences.\n",
    "\n",
    "Each task has a dataset split as train, validation and testing, with the exception that\n",
    "MNLI and AX share the same training set.\n",
    "\n",
    "***All except \"STSB\" can be viewed as a text classification task, while \"STSB\" is a\n",
    "text regression task (output a float number in range [0, 5]).***\n",
    "\n",
    "The common approach to use GLUE benchmark is to build your model and train/finetune it on\n",
    "the training set, and evaluate locally with the validation set. Once you are satisfied\n",
    "with the training/validation results, generate predictions on the testing set, and write\n",
    "your predictions to `*.tsv` file (e.g., mrpc.tsv) with the required format. Then you\n",
    "submit a zip file with all `.tsv` files to GLUE leaderboard, then the web will tell you\n",
    "the actual performance on testing dataset. You cannot evaluate on testing dataset locally\n",
    "because the testing label is not publicized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Install/Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install -q keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Get GLUE Dataset and Preprocess Data\n",
    "This section discusses about how to download GLUE dataset in a Tensorflow runtime, and\n",
    "preprocess the dataset so that it's ready for training.\n",
    "\n",
    "Let's first define the task we will evaluate. In this guide I am using `mrpc` as a\n",
    "showcase, but you can change it to any glue task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "task_name = \"mrpc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then we define a few hyperparameters necessary for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Download GLUE Dataset\n",
    "\n",
    "We download GLUE dataset from Tensorflow Datasets (TFDS). The nice thing is the\n",
    "downloaded dataset is already of type `tf.data.Dataset`, which has good support for\n",
    "parallelism, accelerator optmization and etc. Relative materials can be found\n",
    "[here](https://www.tensorflow.org/datasets/overview).\n",
    "\n",
    "***AX, MNLI_Matched and MNLI_Mismatched*** are special because they share the same\n",
    "training dataset, while they all have its own testing dataset. MNLI_Matched and\n",
    "MNLI_Mismatched also have their own validation dataset, while AX provides no validation\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "if task_name in (\"ax\", \"mnli_matched\", \"mnli_mismatched\"):\n",
    "    train_ds, validation_ds = tfds.load(\n",
    "        \"glue/mnli\",\n",
    "        split=[\"train\", \"validation_matched\"],\n",
    "    )\n",
    "    if task_name == \"ax\":\n",
    "        test_ds = tfds.load(\n",
    "            \"glue/ax\",\n",
    "            split=\"test\",\n",
    "        )\n",
    "\n",
    "    if task_name == \"mnli_matched\":\n",
    "        test_ds = tfds.load(\n",
    "            \"glue/mnli_matched\",\n",
    "            split=\"test\",\n",
    "        )\n",
    "\n",
    "    if task_name == \"mnli_mismatched\":\n",
    "        validation_ds, test_ds = tfds.load(\n",
    "            \"glue/ax\",\n",
    "            split=[\n",
    "                \"validation\",\n",
    "                \"test\",\n",
    "            ],\n",
    "        )\n",
    "else:\n",
    "    train_ds, test_ds, validation_ds = tfds.load(\n",
    "        f\"glue/{task_name}\",\n",
    "        split=[\"train\", \"test\", \"validation\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Save The Testing Data Index Order\n",
    "\n",
    "This is required for generating leaderboard submission file. You can skip reading this\n",
    "code for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "idx_order = test_ds.map(lambda data: data[\"idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Data Unification\n",
    "\n",
    "GLUE datasets come in dictionary format, and each task has its own feature name, such as\n",
    "\"sentence1\" and \"premise\". This data format discrepancy creates complexity on our\n",
    "training, so we standardize the format to simplify the training.\n",
    "\n",
    "For all tasks, after standardization, each data record will have the following format:\n",
    "`(features, label)`, while `features` is a tuple of one element if the task has only one\n",
    "feature, e.g., \"COLA\", and is a tuple of 2 elements if the tasks has 2 featyres, e.g.,\n",
    "\"MRPC\". There are no GLUE tasks having >2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Get the feature names for our selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "FEATURES = {\n",
    "    \"cola\": (\"sentence\",),\n",
    "    \"sst2\": (\"sentence\",),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli_matched\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli_mismatched\": (\"premise\", \"hypothesis\"),\n",
    "    \"ax\": (\"premise\", \"hypothesis\"),\n",
    "    \"qnli\": (\"question\", \"question\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "}\n",
    "\n",
    "feature_names = FEATURES[task_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define the function doing the standardization - convert the dictionary into (features,\n",
    "label) format. Then use `map` function to apply the standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_features(x):\n",
    "    features = tuple([x[name] for name in feature_names])\n",
    "    label = x[\"label\"]\n",
    "    return (features, label)\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(split_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(split_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_ds = validation_ds.map(split_features, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Tokenization and Packing\n",
    "\n",
    "NLP models cannot directly work on text input, we need to convert the text input to float\n",
    "vectors. Here we use `keras_nlp.models.BertTokenizer` to do the conversion.\n",
    "\n",
    "Remember our `feature` can be a tuple of 2 strings, we need some way to combine them\n",
    "together. A common approach is to have a `[SEP]` token between two sentences, and put two\n",
    "special token separately at the beginning and end of the combined sentence. For a unified\n",
    "workflow, if the feature has only one string, we skip the `[SEP]` token, but still pad\n",
    "the start and end token. This can be easily approached by\n",
    "`keras_nlp.layers.MultiSegmentPacker`, as shown by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_base_en_uncased\")\n",
    "\n",
    "packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "    start_value=tokenizer.cls_token_id,\n",
    "    end_value=tokenizer.sep_token_id,\n",
    "    pad_value=tokenizer.pad_token_id,\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_fn(feature, label):\n",
    "    tokenized_data = [tokenizer(x) for x in feature]\n",
    "    token_ids, _ = packer(tokenized_data)\n",
    "    padding_mask = token_ids != tokenizer.pad_token_id\n",
    "    return {\"token_ids\": token_ids, \"padding_mask\": padding_mask}, label\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "After applying the `preprocess_fn`, for all GLUE tasks, each data record is a tuple\n",
    "`(features, label)`, and `features` is a dictionary of format\n",
    "```\n",
    "{\n",
    "    \"token_ids\": a tf.Tensor representing the token ids.\n",
    "    \"padding_mask\": a tf.Tensor representing the mask (0 means the position is masked).\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds_processed = (\n",
    "    train_ds.map(preprocess_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "validation_ds_processed = (\n",
    "    validation_ds.map(preprocess_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_ds_processed = (\n",
    "    test_ds.map(preprocess_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define The Model And Set Up Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's define some hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "if task_name == \"stsb\":\n",
    "    num_classes = 1\n",
    "elif task_name in (\n",
    "    \"mnli\",\n",
    "    \"mnli_mismatched\",\n",
    "    \"mnli_matched\",\n",
    "    \"ax\",\n",
    "):\n",
    "    num_classes = 3\n",
    "else:\n",
    "    num_classes = 2\n",
    "\n",
    "feature_dim = 128\n",
    "transformer_intermediate_dim = 128\n",
    "vocab_size = tokenizer.vocabulary_size()\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then we define the classification model, it's a simple Transformer encoder with one\n",
    "transformer layer and one dense layer. We can build this model with a few lines with\n",
    "KerasNLP offerings.\n",
    "\n",
    "We build the model using Keras functional API, at a high level it's to define a symbolic\n",
    "input, and define your graph to compute the symbolic output, then tell a `keras.Model`\n",
    "the input and output information. For more details, please refer to [this\n",
    "guide](https://keras.io/guides/functional_api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "token_id_input = keras.Input(shape=(None,), dtype=\"int32\", name=\"token_ids\")\n",
    "padding_mask = keras.Input(shape=(None,), dtype=\"int32\", name=\"padding_mask\")\n",
    "x = keras.layers.Embedding(tokenizer.vocabulary_size(), feature_dim)(token_id_input)\n",
    "x = keras_nlp.layers.TransformerEncoder(\n",
    "    transformer_intermediate_dim, 4, activation=\"tanh\"\n",
    ")(x, padding_mask=padding_mask)[:, 0, :]\n",
    "x = keras.layers.Dense(num_classes, activation=\"tanh\")(x)\n",
    "\n",
    "inputs = {\n",
    "    \"token_ids\": token_id_input,\n",
    "    \"padding_mask\": padding_mask,\n",
    "}\n",
    "outputs = x\n",
    "classification_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define loss function and metrics to track training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "if task_name == \"stsb\":\n",
    "    loss = keras.losses.MeanSquaredError()\n",
    "    metrics = [keras.metrics.MeanSquaredError()]\n",
    "else:\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "classification_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "classification_model.fit(\n",
    "    train_ds_processed,\n",
    "    validation_data=train_ds_processed,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Generate GLUE Leaderboard Submission Files\n",
    "\n",
    "Now we have a trained model! Let's use it to evaluate the testing dataset, and generate\n",
    "the leaderboard submission file.\n",
    "\n",
    "First we define the corresponding file name and label names for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"cola\": \"CoLA.tsv\",\n",
    "    \"sst2\": \"SST-2.tsv\",\n",
    "    \"mrpc\": \"MRPC.tsv\",\n",
    "    \"qqp\": \"QQP.tsv\",\n",
    "    \"stsb\": \"STS-B.tsv\",\n",
    "    \"mnli_matched\": \"MNLI-m.tsv\",\n",
    "    \"mnli_mismatched\": \"MNLI-mm.tsv\",\n",
    "    \"qnli\": \"QNLI.tsv\",\n",
    "    \"rte\": \"RTE.tsv\",\n",
    "    \"wnli\": \"WNLI.tsv\",\n",
    "    \"ax\": \"AX.tsv\",\n",
    "}\n",
    "\n",
    "labelnames = {\n",
    "    \"mnli_matched\": [\"entailment\", \"neutral\", \"contradiction\"],\n",
    "    \"mnli_mismatched\": [\"entailment\", \"neutral\", \"contradiction\"],\n",
    "    \"ax\": [\"entailment\", \"neutral\", \"contradiction\"],\n",
    "    \"qnli\": [\"entailment\", \"not_entailment\"],\n",
    "    \"rte\": [\"entailment\", \"not_entailment\"],\n",
    "}\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Create an empty file now, we will fill the content soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "submission_directory = \"glue_submissions\"\n",
    "if not os.path.exists(submission_directory):\n",
    "    os.makedirs(submission_directory)\n",
    "filename = submission_directory + \"/\" + filenames[task_name]\n",
    "labelname = labelnames.get(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Use our model to generate the predictions, then we map the prediction to the right index\n",
    "order. We previously created `idx_order`, now it's coming to the stage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "predictions = classification_model.predict(test_ds_processed)\n",
    "if task_name == \"stsb\":\n",
    "    predictions = np.squeeze(predictions)\n",
    "else:\n",
    "    predictions = np.argmax(predictions, -1)\n",
    "\n",
    "# Map the predictions to the right index order.\n",
    "idx_order = list(idx_order.as_numpy_iterator())\n",
    "contents = [\"\" for _ in idx_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The last step is to do the right formatting. Some tasks have label in integers, while\n",
    "some tasks have its special string labels such as \"entailment\" and \"not_entailment\" in\n",
    "QNLI. We also write the required headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for idx, pred in zip(idx_order, predictions):\n",
    "    if labelname:\n",
    "        pred_value = labelname[int(pred)]\n",
    "    else:\n",
    "        pred_value = pred\n",
    "        if task_name == \"stsb\":\n",
    "            pred_value = min(pred_value, 5)\n",
    "            pred_value = max(pred_value, 0)\n",
    "            pred_value = f\"{pred_value:.3f}\"\n",
    "    contents[idx] = pred_value\n",
    "\n",
    "with tf.io.gfile.GFile(filename, \"w\") as f:\n",
    "    # GLUE requires a format of index + tab + prediction.\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    # Write the required headline for GLUE.\n",
    "    writer.writerow([\"index\", \"prediction\"])\n",
    "\n",
    "    for idx, value in enumerate(contents):\n",
    "        writer.writerow([idx, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Assume you have `task_name=mrpc`, now you can check its content with the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!head -10 glue_submissions/MRPC.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For a real submission, you have to make a zip file including all tasks. If you just want\n",
    "to evaluate on a single task on testing dataset, you can download the sample submission,\n",
    "and replace the corresponding submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!curl -O https://gluebenchmark.com/assets/CBOW.zip\n",
    "!!unzip -d sample_submissions/ CBOW.zip\n",
    "!!cp glue_submissions/MRPC.tsv sample_submissions/\n",
    "!!zip -r submission.zip . -i sample_submissions/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You can download the generated `submission.zip` file to your local disk, and submit it\n",
    "via the [official portal](https://gluebenchmark.com/submit). The score will be available\n",
    "~30s after submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Congrats!! You have reached the end of the guide, hope you now have a good understanding\n",
    "of GLUE benchmark and how to use it. Again if you are looking for something just working,\n",
    "please check out the [GLUE\n",
    "script](https://github.com/keras-team/keras-nlp/tree/master/examples/glue_benchmark)\n",
    "available in KerasNLP."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "glue_benchmark",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}